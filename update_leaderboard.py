import requests
import json
import time
import logging
import os
from datetime import datetime, timedelta, timezone

logging.basicConfig(level=logging.INFO, format="%(asctime)s %(levelname)s: %(message)s")

API_KEY = os.getenv("API_KEY")
COMMUNITY_ID = "1902883093062574425"
BASE_URL = f"https://api.socialdata.tools/twitter/community/{COMMUNITY_ID}/tweets"
HEADERS = {"Authorization": f"Bearer {API_KEY}"}

TWEETS_FILE = "all_tweets.json"
LEADERBOARD_FILE = "leaderboard.json"

def is_within_last_n_days(created_at_str, days=60):
    """
    –ü—Ä–æ–≤–µ—Ä—è–µ—Ç, –±—ã–ª–∞ –ª–∏ –¥–∞—Ç–∞ —Å–æ–∑–¥–∞–Ω–∏—è —Ç–≤–∏—Ç–∞ (–≤ —Ñ–æ—Ä–º–∞—Ç–µ ISO 8601) –≤ —Ç–µ—á–µ–Ω–∏–µ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö N –¥–Ω–µ–π.
    """
    try:
        tweet_time = datetime.fromisoformat(created_at_str.replace("Z", "+00:00"))
    except ValueError:
        logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {created_at_str}")
        return False

    now = datetime.now(timezone.utc)
    n_days_ago = now - timedelta(days=days)

    return tweet_time >= n_days_ago

def load_json(path):
    try:
        with open(path, "r", encoding="utf-8") as f:
            return json.load(f)
    except FileNotFoundError:
        return []

def save_json(path, data):
    with open(path, "w", encoding="utf-8") as f:
        json.dump(data, f, ensure_ascii=False, indent=2)

def fetch_tweets(cursor=None, limit=50):
    params = {"type": "Latest", "limit": limit}
    if cursor:
        params["cursor"] = cursor
    r = requests.get(BASE_URL, headers=HEADERS, params=params)
    r.raise_for_status()
    return r.json()

def collect_all_tweets():
    all_tweets = []
    seen_ids = set()

    cursor = None
    total_new = 0
    while True:
        data = fetch_tweets(cursor)
        tweets = data.get("tweets", [])
        cursor = data.get("next_cursor")

        if not tweets:
            break

        new_tweets = [t for t in tweets if t["id_str"] not in seen_ids and is_within_last_n_days(t.get("created_at"), days=60)]

        if not new_tweets:
            logging.info("–î–æ—Å—Ç–∏–≥–Ω—É—Ç—ã —Ç–≤–∏—Ç—ã –∑–∞ –ø—Ä–µ–¥–µ–ª–∞–º–∏ –ø–æ—Å–ª–µ–¥–Ω–∏—Ö 2 –º–µ—Å—è—Ü–µ–≤, –æ—Å—Ç–∞–Ω–æ–≤–∫–∞ —Å–±–æ—Ä–∞.")
            break

        all_tweets.extend(new_tweets)
        seen_ids.update(t["id_str"] for t in new_tweets)
        total_new += len(new_tweets)

        logging.info(f"‚úÖ –ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(new_tweets)} –Ω–æ–≤—ã—Ö —Ç–≤–∏—Ç–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –º–µ—Å—è—Ü–∞ (–≤—Å–µ–≥–æ: {len(all_tweets)})")

        if not cursor:
            break

        time.sleep(3)

    save_json(TWEETS_FILE, all_tweets)
    logging.info(f"\n–°–±–æ—Ä –∑–∞–≤–µ—Ä—à—ë–Ω. –í—Å–µ–≥–æ —Ç–≤–∏—Ç–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –º–µ—Å—è—Ü–∞: {len(all_tweets)}")
    return all_tweets

def build_leaderboard(tweets):
    leaderboard = {}

    for t in tweets:
        user = t.get("user")
        if not user:
            continue
        name = user.get("screen_name")
        if not name:
            continue

        stats = leaderboard.setdefault(name, {
            "posts": 0,
            "likes": 0,
            "retweets": 0,
            "comments": 0,
            "quotes": 0,
            "views": 0
        })

        stats["posts"] += 1
        stats["likes"] += t.get("favorite_count", 0)
        stats["retweets"] += t.get("retweet_count", 0)
        stats["comments"] += t.get("reply_count", 0)
        stats["quotes"] += t.get("quote_count", 0)
        stats["views"] += t.get("views_count", 0)

    leaderboard_list = [[user, stats] for user, stats in leaderboard.items()]
    save_json(LEADERBOARD_FILE, leaderboard_list)
    logging.info(f"üèÜ –õ–∏–¥–µ—Ä–±–æ—Ä–¥ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(leaderboard_list)} —É—á–∞—Å—Ç–Ω–∏–∫–æ–≤ –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –º–µ—Å—è—Ü–∞).")

def build_daily_stats(tweets):
    """
    –°–æ–±–∏—Ä–∞–µ—Ç —Å—Ç–∞—Ç–∏—Å—Ç–∏–∫—É –ø–æ –¥–Ω—è–º: —Å–∫–æ–ª—å–∫–æ –ø–æ—Å—Ç–æ–≤ –±—ã–ª–æ –æ–ø—É–±–ª–∏–∫–æ–≤–∞–Ω–æ –≤ –∫–∞–∂–¥—ã–π –¥–µ–Ω—å (–∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –º–µ—Å—è—Ü–∞).
    """
    daily_stats = {}

    for t in tweets:
        created_at_str = t.get("created_at")
        if not created_at_str:
            continue

        try:
            tweet_date = datetime.fromisoformat(created_at_str.replace("Z", "+00:00")).date()
        except ValueError:
            logging.warning(f"–ù–µ–≤–µ—Ä–Ω—ã–π —Ñ–æ—Ä–º–∞—Ç –¥–∞—Ç—ã: {created_at_str}")
            continue

        daily_stats[tweet_date] = daily_stats.get(tweet_date, 0) + 1

    daily_list = [{"date": str(date), "posts": count} for date, count in sorted(daily_stats.items())]
    save_json("daily_posts.json", daily_list)
    logging.info(f"üìä –ì—Ä–∞—Ñ–∏–∫ –∞–∫—Ç–∏–≤–Ω–æ—Å—Ç–∏ –æ–±–Ω–æ–≤–ª—ë–Ω ({len(daily_list)} –¥–Ω–µ–π –∑–∞ –ø–æ—Å–ª–µ–¥–Ω–∏–µ 2 –º–µ—Å—è—Ü–∞).")

if __name__ == "__main__":
    tweets = collect_all_tweets()
    build_leaderboard(tweets)
    build_daily_stats(tweets)

